{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11afed6e-38ec-4dde-9b76-aa5343ac1d7e",
   "metadata": {
    "id": "11afed6e-38ec-4dde-9b76-aa5343ac1d7e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fahrmodell trainieren\n",
    "\n",
    "In diesem Notebook werden wir unser Fahrmodell f√ºr das selbst√§ndige Fahren trainieren.\n",
    "\n",
    "Du wirst hier viele Gemeinsamkeiten zur Bilderkennung von letzter Woche sehen, aber auch einige Unterschiede.\n",
    "\n",
    "√úberlege dir nochmals, was wir als Input ben√∂tigen und was unser Output *\"Label\"* ist. Also was das Modell vorhersagen soll.\n",
    "\n",
    "‚ö†Ô∏è **In diesem Notebook befinden Aufgaben. Ihr m√ºsst die gel√∂sten Aufgaben eurem Coach abgeben, damit sie bewertet werden.** ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783b6495c811703",
   "metadata": {},
   "source": [
    "Zu Beginn importieren wir einige Packages und testen ob eine GPU zur verf√ºgung steht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae75c1-c131-4f74-8976-bdf9846702bd",
   "metadata": {
    "id": "08ae75c1-c131-4f74-8976-bdf9846702bd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelSummary\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from aic.logger import DictLogger\n",
    "from aic.runs import create_run_dir\n",
    "from aic.helpers import disable_warnings\n",
    "disable_warnings()\n",
    "\n",
    "print(f'GPU (Cuda) is available: {torch.cuda.is_available()}')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.mps.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9688709",
   "metadata": {
    "id": "d9688709"
   },
   "source": [
    "## Daten laden\n",
    "\n",
    "Die aufgenommen Fahrdaten werden eingelesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e6de1",
   "metadata": {},
   "source": [
    "### Daten laden\n",
    "Um die Daten laden zu k√∂nnen ben√∂tigen wir einige Hilfsfunktionen. Die erste ist eine Funktion um alle Bilder im Ordner zu finden und als Pfade zur√ºckzugeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef937c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(path: Path) -> list:\n",
    "    return list(sorted(path.glob('**/*.jpg')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7225be2",
   "metadata": {},
   "source": [
    "### üìù Aufgabe 3.2\n",
    "Die n√§chste Funktion liest die Daten f√ºr Lenkung und Geschwindigkeit aus dem zum Bild geh√∂renden JSON-File heraus. Erstelle eine Funktion, die f√ºr jedes Bild `angle` und `speed` zur√ºckgibt.\n",
    "\n",
    "- Als Eingabe erh√§lt die Funktion einen Bild-Pfad, z.B. `recordings/01/01.jpg`\n",
    "- Die JSON-Datei hat den gleichen Pfad, einfach mit der Endung `.json` anstatt `.jpg`\n",
    "- Die Funktion gibt ein Tuple `(angle, speed)` mit den Werten als `float` zur√ºck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54d309",
   "metadata": {
    "id": "0f54d309"
   },
   "outputs": [],
   "source": [
    "# üìù Programmiere die Funktion:\n",
    "def read_data_from_json(image_path: str | Path) -> tuple[float, float]:\n",
    "    path = Path(image_path).with_suffix('.json')\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    steering_angle = float(data['angle'])\n",
    "    throttle = float(data['speed'])\n",
    "    return steering_angle, throttle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662f04adf99baa1",
   "metadata": {},
   "source": [
    "Um deinen Code zu √ºberpr√ºfen kannst du folgende Tests ausf√ºhren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "import tempfile\n",
    "p = Path(tempfile.gettempdir()) / 'test1-3'; p.mkdir(exist_ok=True)\n",
    "img_path = p / 'test_01.jpg'\n",
    "img_path.touch()\n",
    "json_path = p / 'test_01.json'\n",
    "json_path.write_text('{\"angle\": 17.805982737, \"speed\": 31}')\n",
    "\n",
    "# Test\n",
    "ret = read_data_from_json(str(img_path))\n",
    "assert isinstance(ret, tuple), f'Die Funktion sollte ein Tuple zur√ºckgeben, aber gibt {type(ret)} zur√ºck'\n",
    "assert len(ret) == 2, f'Das Tuple sollte genau 2 Elemente enthalten, aber enth√§lt {len(ret)} Elemente'\n",
    "assert all(isinstance(x, float) for x in ret), f'Die Elemente des Tuples sollten Zahlen von Typ float sein'\n",
    "assert ret == (17.805982737, 31), f'Die Funktion sollte (17.805982737, 31.0) zur√ºckgeben, gibt aber {ret} zur√ºck'\n",
    "\n",
    "# Cleanup\n",
    "img_path.unlink()\n",
    "print('Die Funktion funktioniert üëç')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86de146d1ed5aa",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir die Bilder laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8923cb06778a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = Path('../data/')\n",
    "\n",
    "# Sollte der Pfad nicht existieren, wird hier eine Fehlermeldung ausgegeben\n",
    "if not image_dir_path.exists():\n",
    "    raise Exception(f\"Fehler: Der Pfad {image_dir_path} existiert nicht üõë\")\n",
    "else:\n",
    "    print(f\"Der Pfad {image_dir_path} wurde gefunden üëç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b8a57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "1c6b8a57",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1dc5787c-63d2-4a96-9ef7-6fcd6155b6db",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# alle Bildpfade laden\n",
    "image_paths = get_image_paths(image_dir_path)\n",
    "assert len(image_paths) > 0, \"Keine Bilder gefunden\"\n",
    "print(f\"Es wurden {len(image_paths)} Bilder gefunden\")\n",
    "\n",
    "# der erste Pfad ausw√§hlen und ausgeben (wenn du einen anderen willst, √§ndere den Index)\n",
    "some_image_path = image_paths[42]\n",
    "print('Beispielbild:', some_image_path)\n",
    "\n",
    "# Bild laden und anzeigen\n",
    "img = Image.open(some_image_path)\n",
    "print(f\"Gr√∂√üe des Bildes (BxH): {img.size}\")\n",
    "display(img)\n",
    "\n",
    "# Dazugeh√∂rige Daten f√ºr Lenkung und Geschwindigkeit ausgeben\n",
    "angle, speed = read_data_from_json(some_image_path)\n",
    "print(f\"Geschwindigkeit {speed:.2f}, Lenkung: {angle:.2f}\")\n",
    "\n",
    "sorted_image_paths = get_image_paths(image_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c2c0d",
   "metadata": {
    "id": "324c2c0d"
   },
   "source": [
    "### Daten betrachten\n",
    "\n",
    "Wir k√∂nnen uns nun die Daten betrachten, auf welche wir Trainieren wollen. In diesem Schritt k√∂nnen wir sch√§dliche Daten erkennen und entfernen\n",
    "\n",
    "*Wichtig*: Wenn du hier unlogische Daten hast, dann entferne diese Abschnitte aus den Trainingsdaten. Beispiel hierf√ºr sind:\n",
    "- Minus-Geschwindigkeit (unser Modell soll nie r√ºckw√§rts fahren)\n",
    "- L√§ngere Abschnitte mit Null-Geschwindigkeit (unser Fahrmodell soll nie anhalten)\n",
    "- Wenn der Durchschnitt der Lenkung um 0 herum ist, heisst das, dass du vermutlich gleichviel Links- wie Rechtskurven in den Daten hast\n",
    "\n",
    "**Dein Modell kann nur so gut sein wie die Daten mit denen du es trainiertes!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed6caee9076bf3",
   "metadata": {},
   "source": [
    "Wir k√∂nnen uns alle Fahrdaten als Diagramm betrachten und einige Kennzahlen ermitteln.\n",
    "\n",
    "*Hineweis: Das auslesen der Daten kann eine Weile dauern.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce8cc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbce8cc3",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "377949a7-9494-4625-ccbc-5b9bdab41bc8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "with ThreadPool(20) as pool:\n",
    "  vehicle_data = np.array(pool.map(read_data_from_json, image_paths))\n",
    "all_angles = vehicle_data[:,0]\n",
    "all_speeds = vehicle_data[:,1]\n",
    "\n",
    "print(f\"Kennzahlen zu Lenkung: min={all_angles.min():.2f}, max: {all_angles.max():.2f}, Durchschnitt: {all_angles.mean():.2f}\")\n",
    "print(f\"Kennzahlen zu Geschwindigkeit: min={all_speeds.min():.2f}, max: {all_speeds.max():.2f}, Durchschnitt: {all_speeds.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9a997af11b21b",
   "metadata": {},
   "source": [
    "Als n√§chstes plotten wir den Winkel und die Geschwindigkeit √ºber die Zeit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48PCnEPW6lgE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "48PCnEPW6lgE",
    "outputId": "f0440ce1-876a-4cd5-ade4-cb36573ff93b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.style as mplstyle\n",
    "mplstyle.use('fast')\n",
    "fig, (ax1, ax2) = plt.subplots(2,figsize=(10, 8))\n",
    "ax1.plot(all_angles)\n",
    "ax1.set_title(\"Angle\")\n",
    "ax1.minorticks_on()\n",
    "ax2.plot(all_speeds)\n",
    "ax2.set_title(\"Speed\")\n",
    "ax2.minorticks_on()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0f0e6",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Beim trainieren von Machine-Learning-Modellen sind die Trainingsdaten oft der Hauptfaktor, ob das Modell das gew√ºnschte Wissen lernt oder nicht. Gleichzeitig ist das sammeln von Daten Zeitaufw√§ndig. Um aus einer begrenzten Menge Daten mehr zu machen, verwenden wir Augmentation. Augmentation heisst \"Erweiterung\". Ziel ist es dabei unsere Bilder leicht zu ver√§ndern, um so mehr Trainingsdaten zu erhalten, ohne tats√§chlich mehr Bilder aufzunehmen. Augmentation bei Bildern kann z.B. die Helligkeit und Kontrast ver√§ndern, oder das Bild leicht drehen oder vergr√∂ssern.\n",
    "\n",
    "All das hilft, dass unsere AI nicht die Trainingsdaten auswendig lernt (Overfitting), sondern lernt zu \"generalisieren\". Also anstatt, dass die AI lernt *\"wenn der 7 Pixel von Links Blau ist...\"*, soll die AI lernen *\"wenn ich eine Form erkenne, die wie eine Linkskurve aussieht...\"*.\n",
    "\n",
    "Du kannst [hier](https://www.arocom.de/fachbegriffe/kuenstliche-intelligenz/datenaugmentierung) mehr √ºber Data Augmentation nachlesen und dir [hier](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py) die Data Augmentation Funktionen von PyTorch ansehen. **Eine sch√∂ne √úbersicht** √ºber die in PyTorch enthaltenen Augmentation-Funktionen findest du [hier](https://pytorch.org/vision/2.0/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py).\n",
    "\n",
    "F√ºr erste werden wir mit der Helligkeit und Farbe im Bild augmentieren, damit unser Modell auch zurechtkommt, wenn die Lichtverh√§ltnisse leicht anders sind als im Training. Sp√§ter (siehe zus√§tzliche Aufgaben) kannst du auch eigene Augmentations vornehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "transformer = v2.Compose(\n",
    "    [\n",
    "        # Helligkeit, Kontrast, S√§ttigung und Farbton werden zuf√§llig bis zum angegeben Wert ge√§ndert\n",
    "        # Du kannst die Werte anpassen, wenn du willst (siehe Zusatzaufgabe)\n",
    "        v2.ColorJitter(\n",
    "            brightness=0.5,\n",
    "            saturation=0.2,\n",
    "            hue=0.1,\n",
    "        ),\n",
    "    v2.ColorJitter(brightness=0.5, saturation=0.2, hue=0.2),\n",
    "\n",
    "    v2.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    # v2.Resize((120, 160)),\n",
    "    v2.ElasticTransform(alpha=100),\n",
    "    v2.RandomInvert(),\n",
    "    v2.RandomPosterize(bits=4, p=0.3),\n",
    "    v2.RandomSolarize(threshold=128, p=0.3),\n",
    "    v2.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419810b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sehen wir uns an, was die Augmentation mit dem Bild macht\n",
    "# (da die Transformation zuf√§llig ist, kannst du die Zelle mehrmals ausf√ºhren um verschiedene Ergebnisse zu sehen)\n",
    "\n",
    "# Wir verwenden unser Beispiel-Bild von weiter oben\n",
    "fig, axs = plt.subplots(1, 4, figsize=(3*4, 3))\n",
    "for i, ax in enumerate(axs):\n",
    "    # Zuerst das Originalbild\n",
    "    if i == 0:\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(\"Original\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "    # Danach 3 zus√§tzlich augmentierte Bilder\n",
    "    else:\n",
    "        augmented_img = transformer(img)\n",
    "        ax.imshow(augmented_img)\n",
    "        ax.set_title(f\"Augmented {i}\")\n",
    "        ax.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7123c",
   "metadata": {},
   "source": [
    "Unser Transformer werden wir anschliessend in im Trainings-Datenset einbauen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8c099",
   "metadata": {
    "id": "49d8c099"
   },
   "source": [
    "### Datasets und Datenloader erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95af07",
   "metadata": {
    "id": "cf95af07",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "MEM_SIZE = 8\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, sorted_image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.sorted_image_paths = sorted_image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        image = Image.open(path)\n",
    "        # Wir werden das Bild etwas verkleinern, um Rechenzeit zu sparen (es sind immer noch gen√ºgend Informationen im Bild vorhanden)\n",
    "        # Original ist das Bild BxH 320x240 Pixel gross, wir verkleinern es auf 16x120 Pixel\n",
    "        image = image.resize((160, 120), resample=Image.Resampling.NEAREST)\n",
    "        # Augmentation (wenn vorhanden)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        image_array = (np.asarray(image) / 255.0).astype(np.float32).transpose(2, 0, 1)\n",
    "        image_tensor = torch.tensor(image_array)\n",
    "        angle, _ = read_data_from_json(path)\n",
    "        angle_tensor = torch.tensor(angle, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        last_angles = []\n",
    "\n",
    "        data_index = self.sorted_image_paths.index(path)\n",
    "        start_idx = max(0, data_index - MEM_SIZE)\n",
    "        current_run = path.name.split('_')[0]\n",
    "        start_idx_run = self.sorted_image_paths[start_idx].name.split('_')[0]\n",
    "        if current_run != start_idx_run:\n",
    "            start_idx = data_index - int(path.stem.split('_')[1])\n",
    "\n",
    "        angle_paths = self.sorted_image_paths[start_idx:data_index]\n",
    "        for angle_path in angle_paths:\n",
    "            angle_value, _ = read_data_from_json(angle_path)\n",
    "            last_angles.insert(0, angle_value)\n",
    "\n",
    "        if len(last_angles) < MEM_SIZE:\n",
    "            last_angles += [0.0] * (MEM_SIZE - len(last_angles)) \n",
    "        else:\n",
    "            last_angles = last_angles[:MEM_SIZE]\n",
    "\n",
    "        angle_history_tensor = torch.tensor(last_angles, dtype=torch.float32)\n",
    "\n",
    "        return image_tensor, angle_tensor, angle_history_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93310959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Wir werden 80% der Daten f√ºr das Training verwenden und 20% f√ºr das Validieren\n",
    "random.shuffle(image_paths)\n",
    "split_idx = int(len(image_paths) * 0.8)\n",
    "train_image_paths = image_paths[:split_idx]\n",
    "val_image_paths = image_paths[split_idx:]\n",
    "\n",
    "# Wichtig: Augmentation nur f√ºr das Trainingsset aktivieren!\n",
    "train_dataset = ImageDataset(train_image_paths, sorted_image_paths, transform=transformer)\n",
    "val_dataset = ImageDataset(val_image_paths, sorted_image_paths, transform=None)\n",
    "\n",
    "# √úberpr√ºfen wir mal die Gr√∂sse dieser Sets\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "\n",
    "# Inhalt anschauen\n",
    "image, angle, last_angles = train_dataset[0]\n",
    "print(image.shape, image.dtype, angle, angle.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der DataLoaders\n",
    "BATCH_SIZE = 128  # Increased for better GPU utilization\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,  # Speeds up CPU->GPU transfer\n",
    "    persistent_workers=True  # Keeps workers alive between epochs\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    " \n",
    "# Ein Batch aus dem DataLoader laden und dessen Shape anzeigen\n",
    "images, labels, last_angles = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff2d477",
   "metadata": {
    "id": "6ff2d477"
   },
   "source": [
    "## Model definieren & trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7674fd6aa8855e",
   "metadata": {},
   "source": [
    "Vor dem Training erstellen wir einen Ordner in den wir das Modell sowie die Auswertung speichern k√∂nnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc1413f6630747",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_directory = create_run_dir('../runs/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6322140",
   "metadata": {
    "collapsed": false,
    "id": "f6322140",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model definieren\n",
    "\n",
    "F√ºrs Erste werden wir ein Neuronale Netzwerk vorgeben. Es besteht aus mehreren Schichten Convolutional Layers am Anfang und einem Dense Neuronal Netzwerk danach.\n",
    "\n",
    "(Du darfst das Netzwerk anpassen wenn du willst. F√ºr den Anfang empfehlen wir aber mit dieser Architektur zu arbeiten.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0992f1e148db20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriveModel(nn.Module):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "\n",
    "        # Convolutional layers for image processing\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                conv1 = nn.Conv2d(3, 18, 5, stride=2, padding=1),\n",
    "                batch1 = nn.BatchNorm2d(18),\n",
    "                relu1 = nn.ReLU(),\n",
    "\n",
    "                conv2 = nn.Conv2d(18, 32, 5, stride=2, padding=1),\n",
    "                batch2 = nn.BatchNorm2d(32),\n",
    "                relu2 = nn.ReLU(),\n",
    "\n",
    "                conv3 = nn.Conv2d(32, 64, 5, stride=2, padding=1),\n",
    "                batch3 = nn.BatchNorm2d(64),\n",
    "                relu3 = nn.ReLU(),\n",
    "\n",
    "                conv4 = nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "                batch4 = nn.BatchNorm2d(128),\n",
    "                relu4 = nn.ReLU(),\n",
    "\n",
    "                conv5 = nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "                batch5 = nn.BatchNorm2d(64),\n",
    "                relu5 = nn.ReLU(),\n",
    "\n",
    "                one2one = nn.Conv2d(64, 1, 1, stride=1),\n",
    "                flatten = nn.Flatten(1, -1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate the size of flattened conv output\n",
    "        # For 160x120 input: after conv layers you get 266 features (based on your linear1)\n",
    "        # conv_output_size = 266  # You can calculate this or determine it empirically\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 120, 160)\n",
    "            conv_output_size = self.conv_layers(dummy_input).shape[1]\n",
    "        \n",
    "        # Linear layers that take concatenated features\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(conv_output_size + MEM_SIZE, 16, bias=True),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(128, 16),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(16, 1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, image, angle_history):\n",
    "        # Process image through conv layers\n",
    "        conv_features = self.conv_layers(image)\n",
    "        \n",
    "        # Concatenate conv features with angle history\n",
    "        combined = torch.cat([conv_features, angle_history], dim=1)\n",
    "        \n",
    "        # Pass through linear layers\n",
    "        output = self.linear_layers(combined)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3610b071202fc9",
   "metadata": {},
   "source": [
    "Um das Training zu vereinfachen, laden wir unser Pytorch Modell als Pytorch Lightning Modell. Zus√§tzlich erstellen wir einen Logger welche den Loss w√§hrend dem Training als Python Dictionary speichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525069933d0edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "class CustomLightningDriveModel(LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = model.lr\n",
    "        \n",
    "    def forward(self, image, angle_history):\n",
    "        return self.model(image, angle_history)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, angles, angle_histories = batch\n",
    "        predictions = self(images, angle_histories)\n",
    "        loss = nn.functional.mse_loss(predictions, angles)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, angles, angle_histories = batch\n",
    "        predictions = self(images, angle_histories)\n",
    "        loss = nn.functional.mse_loss(predictions, angles)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        y_pred = self(x)\n",
    "        return y_pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "       \n",
    "        # Learning rate scheduler - reduces LR when validation loss plateaus\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "        )\n",
    "       \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "model = DriveModel()\n",
    "lightning_model = CustomLightningDriveModel(model)\n",
    "\n",
    "logger = DictLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6253140d29693",
   "metadata": {},
   "source": [
    "Wir k√∂nnen uns die Architektur des Modells anschauen.\n",
    "Hier sieht man die einzelnen Schichten (Layers), welche Ausgangssignale (Output Shape) sie erzeugen, sowie die Anzahl k√ºnstlicher Neuronen (Parameter), welche sie enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1951450",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelSummary().on_fit_start(Trainer(), lightning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebb509",
   "metadata": {},
   "source": [
    "### Trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f532bec206c02",
   "metadata": {},
   "source": [
    "F√ºr das Training fehlen uns jetzt nur noch die Hyperparameter:\n",
    "- Epochs = Wie viel mal das Modell mit dem ganzen Datensatz trainiert\n",
    "- Learning Rate = Wie stark sich das Modell w√§hrend dem Training anpasst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791e06c",
   "metadata": {
    "id": "e791e06c",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter definieren\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "lightning_model.lr = LEARNING_RATE\n",
    "\n",
    "# Trainer initialisieren mit GPU-Optimierungen\n",
    "trainer = Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[logger],\n",
    "    precision='64-true',  # Mixed precision training for faster computation\n",
    "    accelerator='auto',  # Automatically use available GPU\n",
    "    devices=1,  # Use 1 GPU\n",
    "    accumulate_grad_batches=2,  # Effective batch size = BATCH_SIZE * 2\n",
    "    benchmark=True  # cuDNN benchmark mode for optimal algorithms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bdd69c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "49bdd69c",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "904e6b73-280e-424f-90c2-7422ff578ebf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Die Zeile oben Zeile sorgt daf√ºr, dass unten angezeigt wird, wie lange es gedauert hat, diese Notebook Zelle auszuf√ºhren.\n",
    "\n",
    "# Enable TensorFloat32 for faster matrix operations on newer GPUs\n",
    "torch.set_float32_matmul_precision('highest')  # Beschleunigt das Training auf Kosten leicht geringerer Pr√§zision\n",
    "\n",
    "# Das Training starten. Dieses dauert eine Weile...\n",
    "trainer.fit(lightning_model, training_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd44cdbaa572537",
   "metadata": {},
   "source": [
    "Hinweis: Falls der Fehler `Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm)` auftritt, musst du die BATCH_SIZE reduzieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817cd7a9",
   "metadata": {},
   "source": [
    "### Training anzeigen\n",
    "\n",
    "### üìù Aufgabe 3.3\n",
    "Um das Training beurteilen zu k√∂nnen m√ºssen wir die geloggten Metriken auswerten. In `logger.metrics` sind f√ºr jede Epoche der Validation Loss, und der Trainings Loss gespeichert. Plotte die Metriken mit Matplotlib und vergiss nicht den Plot in der run directory zu speichern.\n",
    "\n",
    "*Tipp: Es kann sein, dass nicht alle Metriken in allen Epochen geloggt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183eb730e6c70c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(metrics: list[dict[str, list[float]]], save_to: str | Path = None):\n",
    "    # üìù Aufgabe: Plotte die Metriken\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Extract the metrics data\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for metric_dict in metrics:\n",
    "        # Handle train loss\n",
    "        if 'train_loss_step' in metric_dict:\n",
    "            loss_value = metric_dict['train_loss_step']\n",
    "            if hasattr(loss_value, 'item'):  # It's a tensor\n",
    "                train_losses.append(loss_value.item())\n",
    "            elif isinstance(loss_value, (list, tuple)):\n",
    "                train_losses.extend([x.item() if hasattr(x, 'item') else x for x in loss_value])\n",
    "            else:\n",
    "                train_losses.append(float(loss_value))\n",
    "                \n",
    "        elif 'train_loss' in metric_dict:\n",
    "            loss_value = metric_dict['train_loss']\n",
    "            if hasattr(loss_value, 'item'):  # It's a tensor\n",
    "                train_losses.append(loss_value.item())\n",
    "            elif isinstance(loss_value, (list, tuple)):\n",
    "                train_losses.extend([x.item() if hasattr(x, 'item') else x for x in loss_value])\n",
    "            else:\n",
    "                train_losses.append(float(loss_value))\n",
    "            \n",
    "        # Handle validation loss\n",
    "        if 'val_loss' in metric_dict:\n",
    "            loss_value = metric_dict['val_loss']\n",
    "            if hasattr(loss_value, 'item'):  # It's a tensor\n",
    "                val_losses.append(loss_value.item())\n",
    "            elif isinstance(loss_value, (list, tuple)):\n",
    "                val_losses.extend([x.item() if hasattr(x, 'item') else x for x in loss_value])\n",
    "            else:\n",
    "                val_losses.append(float(loss_value))\n",
    "    \n",
    "    # Plot the losses\n",
    "    if train_losses:\n",
    "        ax1.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "    if val_losses:\n",
    "        # Validation loss is typically logged less frequently, so we need to spread it out\n",
    "        val_epochs = np.linspace(0, len(train_losses)-1, len(val_losses)) if train_losses else range(len(val_losses))\n",
    "        ax1.plot(val_epochs, val_losses, label='Validation Loss', marker='o', markersize=3)\n",
    "    \n",
    "    ax1.set_title('Training and Validation Loss over Time')\n",
    "    ax1.set_xlabel('Steps/Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot if save_to is provided\n",
    "    if save_to:\n",
    "        plt.savefig(Path(save_to) / 'training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# First, let's examine what's actually in the logger metrics\n",
    "print(\"Available metrics keys:\")\n",
    "if logger.metrics:\n",
    "    print(f\"Number of metric dictionaries: {len(logger.metrics)}\")\n",
    "    for i, metric_dict in enumerate(logger.metrics[:3]):  # Show first 3 for inspection\n",
    "        print(f\"Metric dict {i} keys: {list(metric_dict.keys())}\")\n",
    "\n",
    "plot_training(logger.metrics, save_to=run_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327640796ac7d44",
   "metadata": {},
   "source": [
    "Mit der folgenden Funktion k√∂nnen wir unserem Modell ein Bild geben den Winkel erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor, angle_history_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(img_tensor.unsqueeze(0), angle_history_tensor.unsqueeze(0))\n",
    "    return y_pred.item()\n",
    "\n",
    "# Testen wir unsere Funktion\n",
    "image_tensor, angle_tensor, angle_history_tensor = train_dataset[0]\n",
    "angle_pred = predict(image_tensor, angle_history_tensor)\n",
    "print(f\"Predicted angle: {angle_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b9d004c089732",
   "metadata": {
    "id": "dd7b215b"
   },
   "source": [
    "## Validierung\n",
    "Die Trainingsmetriken zeigen uns schon auf wie gut unser Training geklappt hat. Um einen noch besseren Einblick zu bekommen, wie gut das trainierte Modell tats√§chlich ist, k√∂nnen wir verschiedene Methoden anwenden um unser Modell zu testen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa0f5644b1ac306",
   "metadata": {},
   "source": [
    "### üìù Aufgabe 3.4\n",
    "Als erstes wollen wir den totalen Fehler √ºber das ganze Test-Set berechnen. Schreibe eine Funktion, welche:\n",
    "1. ein datenset entgegen nimmt,\n",
    "2. √ºber das datenset iteriert und f√ºr jedes Bild eine Prediction macht (nutze die `predict` funktion von oben)\n",
    "3. f√ºr jede Prediction den Fehler ausrechnet (`y_pred` - `y_true`)\n",
    "4. die Fehler zusammenz√§hlen\n",
    "5. den total Fehler durch die Anzahl Bilder teilt um den Durchschnittlichen Fehler zu bekommen\n",
    "6. den Fehler zur√ºckgibt und ihn als text file in `save_to` speichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TZBKuQMWkl15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "TZBKuQMWkl15",
    "outputId": "eef0b2e3-67d9-4dd9-9b55-58e6eb4956a3"
   },
   "outputs": [],
   "source": [
    "def calculate_error(data: Dataset, save_to: str | Path = None) -> float:\n",
    "    # üìù Vervollst√§ndige die funktion:\n",
    "    true_y = [angle.item() for _, angle, _ in data]\n",
    "    pred_y = [predict(img, angle_hist) for img, _, angle_hist in data]\n",
    "    errors = [abs(t - p) for t, p in zip(true_y, pred_y)]\n",
    "    mean_error = sum(errors) / len(errors)\n",
    "    #6. den Fehler zur√ºckgibt und ihn als text file in `save_to` speichert.\n",
    "    if save_to is not None:\n",
    "        save_path = Path(save_to) / 'mean_error.txt'\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(f\"{mean_error:.4f}\\n\")\n",
    "        \n",
    "    return mean_error\n",
    "\n",
    "\n",
    "error = calculate_error(val_dataset, save_to=run_directory)\n",
    "print(f\"Der durchschnittliche Fehler auf dem Validierung-Set ist {error:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e998b79",
   "metadata": {},
   "source": [
    "Was sagt uns diese Zahl? Wie gross im Durchschnitt der Winkel-Fehler ist. Eigentlich m√ºssen wir aber auch nur eins wissen: Je geringer, desto besser unser Modell! Der Fehler hilft uns also vor allem verschiedenen Modelle zu vergleichen.\n",
    "\n",
    "Wir k√∂nnen aber auch visuell √ºberpr√ºfen, ob das Modell die Kurven korrekt vorhersagt und bei welchen Werten es M√ºhe hat. Dazu sortieren wir zuerst das Datenset der Gr√∂sse nach. Wenn das Modell gut ist sollten die Linien √ºbereinander liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DKKLYigDlCrw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "DKKLYigDlCrw",
    "outputId": "be544d75-9ff6-433b-ca25-e72354c6a985"
   },
   "outputs": [],
   "source": [
    "def visualize_angels(data: Dataset):\n",
    "    true_y = [angle.item() for _, angle, _ in data]\n",
    "\n",
    "    n = min(100, len(true_y))\n",
    "    true_y_plot = true_y[:n]\n",
    "    pred_y_plot = [predict(image_tensor, angle_history) for image_tensor, _, angle_history in val_dataset][:n]\n",
    "    sorted_idx = np.argsort(true_y_plot)\n",
    "    true_y_plot = np.array(true_y_plot)[sorted_idx]\n",
    "    pred_y_plot = np.array(pred_y_plot)[sorted_idx]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(true_y_plot, label=\"True\")\n",
    "    plt.plot(pred_y_plot, label=\"Predicted\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Angle\")\n",
    "    plt.title(\"Visuelle Inspektion\")\n",
    "    if run_directory:\n",
    "        plt.savefig(Path(run_directory)/'visualize_angels.png')\n",
    "\n",
    "visualize_angels(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e9bee4a0aebc0",
   "metadata": {},
   "source": [
    "Als letzte Analyse wollen wir noch einige Bilder manuell inspizieren. Dazu brauchen wir eine Funktion die einige zuf√§llige Bilder aus einem Datenset ausw√§hlt, durch das Modell laufen l√§sst und danach das Resultat darstellt.\n",
    "\n",
    "### üìù Aufgabe 3.5\n",
    "Schreibe eine Funktion die\n",
    "1. als Argument ein Dataloader und ein `save_to` Pfad entgegen nimmt\n",
    "2. aus dem Dataloader 6 zuf√§llige Bilder ausw√§hlt und in einer Liste speichert\n",
    "3. zu jedem ausgew√§hlten Bild den echten Wert f√ºr den Winkel in einer zweiten Liste speichert\n",
    "4. zu jedem Bild mit der `predict` Funktion eine prediction macht und in einer dritten Liste speichert\n",
    "5. die Bilder zusammen mit `True`, `Pred` und `Fehler` plottet\n",
    "6. den Plot anzeigt und in der `save_to` directory als png speichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fdcaf33d9b31f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "76b674ec-08ad-4cb7-b9f7-885c764090ad",
    "outputId": "6c7b5a4a-7c3b-4774-9167-757d600e08ca",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_drive_predictions(dataloader: DataLoader, save_to: str | Path = None):\n",
    "    # üìù Vervollst√§ndige die funktion:\n",
    "    # 1. Select 6 random images from the dataloader\n",
    "    dataset = dataloader if hasattr(dataloader, '__len__') else dataloader.dataset\n",
    "    n_samples = min(6, len(dataset))\n",
    "    \n",
    "    # Randomly select indices\n",
    "    random_indices = random.sample(range(len(dataset)), n_samples)\n",
    "    \n",
    "    # 2. Store images, true angles, and angle history\n",
    "    images = []\n",
    "    true_angles = []\n",
    "    angle_histories = []\n",
    "    \n",
    "    for idx in random_indices:\n",
    "        image_tensor, angle_tensor, angle_history_tensor = dataset[idx]\n",
    "        images.append(image_tensor)\n",
    "        true_angles.append(angle_tensor.item())\n",
    "        angle_histories.append(angle_history_tensor)\n",
    "    \n",
    "    # 4. Make predictions for each image\n",
    "    predicted_angles = []\n",
    "    for image_tensor, angle_history in zip(images, angle_histories):\n",
    "        pred_angle = predict(image_tensor, angle_history)\n",
    "        predicted_angles.append(pred_angle)\n",
    "    \n",
    "    # 5. Plot the images with True, Pred, and Error values\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Convert tensor to displayable format\n",
    "        img_display = images[i].permute(1, 2, 0).numpy()\n",
    "        \n",
    "        true_angle = true_angles[i]\n",
    "        pred_angle = predicted_angles[i]\n",
    "        error = abs(true_angle - pred_angle)\n",
    "        \n",
    "        axes[i].imshow(img_display)\n",
    "        axes[i].set_title(f'True: {true_angle:.2f}¬∞\\nPred: {pred_angle:.2f}¬∞\\nError: {error:.2f}¬∞')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 6. Save the plot if save_to is provided\n",
    "    if save_to:\n",
    "        save_path = Path(save_to) / 'drive_predictions_visualization.png'\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_drive_predictions(val_dataset, save_to=run_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2809e7-20af-47ac-82b7-e6a190580e81",
   "metadata": {
    "id": "dd2809e7-20af-47ac-82b7-e6a190580e81",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modell Export\n",
    "\n",
    "Damit wir unser Modell auf dem Auto laufen lassen k√∂nnen, m√ºssen wir es exportieren.\n",
    "\n",
    "Dazu speichern wir es im ONNX-Format. Dieses Format komprimiert und optimiert das Model, damit es so schnell wie m√∂glich auf dem Auto l√§uft (fps). Die export funktion ben√∂tigt ein Beispiel Input um die \"shape\" zu speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren, wo das Model gespeichert werden soll. Erstelle den Ordner, falls er noch nicht existiert.\n",
    "\n",
    "model_path = Path(f'../models/drive/{run_directory.name}/DriveModel_v1.onnx')\n",
    "\n",
    "# Zur Sicherheit pr√ºfen, ob das Model bereits existiert, (l√∂sche es, wenn du es √ºberschreiben willst. Oder benenne es um)\n",
    "assert not model_path.exists(), 'Das Model existiert bereits'\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model speichern - mit beiden Inputs (Bild und Angle History)\n",
    "example_image = train_dataset[0][0].unsqueeze(0)\n",
    "example_angle_history = train_dataset[0][2].unsqueeze(0)\n",
    "torch.onnx.export(\n",
    "    model.to('cpu'),\n",
    "    (example_image, example_angle_history),\n",
    "    model_path,\n",
    "    input_names=['image', 'angle_history'],\n",
    "    output_names=['steering_angle'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d50ff79f33bb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
