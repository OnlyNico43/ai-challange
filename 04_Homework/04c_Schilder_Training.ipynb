{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64435dbfc059ba79",
   "metadata": {},
   "source": [
    "# Schilder Model Trainieren\n",
    "Bevor wir beginnen importieren wir wieder einige Packages und testen ob wir eine GPU zur verf√ºgung haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604bade1de01871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aic.helpers import disable_warnings\n",
    "disable_warnings()\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import  matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f'GPU (Cuda) is available: {torch.cuda.is_available()}')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7740d25d541d07",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "Beim trainieren von Machine-Learning-Modellen sind die Trainingsdaten oft der Hauptfaktor ob das Modell das gew√ºnschte Wissen lernt oder nicht. Gleichzeitig ist das sammeln von Daten Zeitaufw√§ndig (teuer) oder zum Teil sogar unm√∂glich (z.B. seltene Krankheit). Eine Technik um aus einem begrenzten Datenset mehr zu machen ist \"Data Augmentation\". In unserem Fall heisst das, dass wir verschiedene Operationen auf unsere gesammelten Bilder anwenden wie zum Beispiel spiegeln oder drehen um auf mehr Bildern zu trainieren. Wichtig ist dass die augmentationen die Bilder genug stark ver√§ndern damit die Bilder f√ºr das Modell nicht gleich aussehen - sonst lassen wir es besser bleiben.\n",
    "\n",
    "F√ºr die augmentation verwenden wir eine Funktion aus pytorch: `torchvision.transforms.v2.Compose[ ... ]`\n",
    "\n",
    "Das folgende Beispiel zeigt eine der m√∂glichen Augmentationen: `RandomRotation(degrees=(0, 180))`\n",
    "\n",
    "![image.png](../assets/pytorch_random_rotation_illustration.png)\n",
    "\n",
    "Die folgenden augmentationen (in pytorch transforms genannt) sind im datalaoder bereits implementiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011ec35b89161c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (320, 240)\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.2),\n",
    "    v2.RandomRotation(degrees=30),\n",
    "    # üìù Deine Augmentationen\n",
    "    v2.Resize(image_size[::-1]),            # skaliert das Bild zur Gr√∂sse die das Modell erwartet\n",
    "    v2.ToImage(),                           # konvertiert das Bild zu einem torchivision.Image\n",
    "    v2.ToDtype(torch.float32, scale=True),  # konvertiert und skaliert das Image zu einem Tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1553eb0e19491",
   "metadata": {},
   "source": [
    "### üìù Aufgabe 4.2\n",
    "Erg√§nze / √§ndere die transforms um die Bilder st√§rker zu augmentieren. Benutz die [√úbersicht](https://docs.pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html) und w√§hle die Augmentationen aus welche f√ºr unsere Schilder-Daten Sinn machen. Beachte dass gewisse Augmentationen die Bilder so ver√§ndern k√∂nnen, dass das Label nicht mehr stimmt und das Modell etwas falsches lernen w√ºrde."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480939cd5c5bcfe",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "Wie beim Fahrmodell brauchen wir ein Python Klasse welche die Daten l√§dt. Der dataloader im `aic_util` package hat\n",
    "folgende Funktionen:\n",
    "- L√§dt alle Bilder aus den Unterordner vom mitgegebenen Pfad wobei das Label dem Namen des Unterordners entspricht\n",
    "- Mischt und balanciert die Bilder falls gew√ºnscht (argument = `True`)\n",
    "- [Augmentiert](https://docs.pytorch.org/vision/master/auto_examples/transforms/plot_transforms_getting_started.html) die Bilder (Spiegeln, verzerren, rotieren, farbanpassungen, unsch√§rfe)\n",
    "- Teilt die Bilder anhand des angebenden Verh√§ltnisses auf ([train / validation split](https://www.v7labs.com/blog/train-validation-test-set))\n",
    "- Konvertiert die Bilder zu Tensoren damit sie von unserem Machine Learning Modell verwendet werden k√∂nnen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b3cc3bc87ec87e",
   "metadata": {},
   "source": [
    "### üìù Aufgabe 4.3\n",
    "Importiere die `load_dataset` Funktion von `aic_util.sign.dataloader` und lade ein test und ein eval\n",
    "datenset von deinen gelabelten Daten. Printe die Gr√∂sse der beiden Datasets. Achtung: die `len` Funktion gibt\n",
    "die Anzahl batches zur√ºck, nicht die Anzahl Bilder.\n",
    "\n",
    "Die Werte f√ºr `split_ratio` und `batch_size` sind sinnvolle default Werte. W√§hrend dem Training kannst du mit diesen\n",
    "Werten herumspielen, finde aber zuerst heraus was sie genau machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b399a5d8aa9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aic.sign.dataloader import load_dataset, ImageDataset\n",
    "\n",
    "data_path = Path('./data/phase-1')\n",
    "split_ratio = 0.8  # wie viel % der Daten sollen im train vs im eval dataset verwendet werden\n",
    "batch_size = 64  # wie viele Bilder in einem Batch (auf einmal) verarbeitet werden\n",
    "\n",
    "# üìù Deine Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9ba5878f238ba",
   "metadata": {},
   "source": [
    "### üìù Aufgabe 4.4\n",
    "Bei Klassifizierung ist es wichtig dass wir gleich viele Bilder pro Klasse haben, sonst lernt das Modell\n",
    "einfach immer die h√§ufigste Klasse vorherzusagen. Die `load_data` Funktion macht das bereits f√ºr uns. Finde den\n",
    "entsprechenden Parameter und setzte ihn oben auf den korrekten Wert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3dd319ca7f956a",
   "metadata": {},
   "source": [
    "## Auswertung\n",
    "Bevor wir unser Modell trainieren brauchen wir noch einige Hilfsfunktionen um das Modell zu beurteilen und auszuwerten.\n",
    "√úberlege dir zuerst folgende Punkte:\n",
    "1. Wie k√∂nnen wir die performance unseres Modells messen, gibt es eine oder mehrere Metriken welche relevant sind?\n",
    "2. Wie k√∂nnen wir diese Metriken visualisieren und vergleichen?\n",
    "3. Welche Zahlen wollen wir erreichen? Wann ist das Modell gut genug?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43691b33b1bc1628",
   "metadata": {},
   "source": [
    "### Bilder darstellen\n",
    "Als erstes erstellen wir eine Funktion welche aus einem Datenset einige zuf√§llige Bilder anzeigt damit wir\n",
    "kontrollieren k√∂nnen ob:\n",
    "- Die richtigen Bilder geladen werden\n",
    "- Die Klassen/Labels stimmen\n",
    "- Die Augmentationen sinnvoll sind\n",
    "\n",
    "Das Dataset gibt `torch.Tensor` zur√ºck. Benutze die `tensor_to_image(...)` funktion um aus dem Tensor eine pixel array zu machen die mit `imshow(...)` anzeigbar ist.\n",
    "\n",
    "### üìù Aufgabe 4.5\n",
    "Implementiere die Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515019f7ea17ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aic.sign.helper import tensor_to_image\n",
    "\n",
    "\n",
    "def show_sign_images(dataset: ImageDataset):\n",
    "    labels = dataset.labels\n",
    "\n",
    "    # üìù Aufgabe: Implementiere die Funktion:\n",
    "\n",
    "\n",
    "show_sign_images(train_data.dataset.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19264321bc37f6b",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "<Bild CM>\n",
    "\n",
    "Ein weitverbreitetes Tool um die predictions (vorhersagen) eines Klassifikationsmodells darzustellen ist die\n",
    "[Confusion Matrix](https://www.datacamp.com/de/tutorial/what-is-a-confusion-matrix-in-machine-learning).\n",
    "\n",
    "### üìù Aufgabe 4.6\n",
    "Vervollst√§ndige die Funktion um eine Confusion Matrix zu plotten. Die Funktion erh√§lt einen [Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) (eine Art Tabelle) mit den folgenden\n",
    "Spalten:\n",
    "- `image`:      Pfad zum Bild\n",
    "- `true`:       Das wahre label\n",
    "- `prediction`: Das vorhergesagte label\n",
    "- `confidence`: Zahl zwischen 0 und 1 wie sicher sich das Modell ist\n",
    "- `output`:     Der gesamte Output des Modells (alle Klassen + confidence) als Python dictionary\n",
    "\n",
    "Speichere die Confusion Matrix an den entsprechenden Ort falls der Funktion ein Pfad mitgegeben wird.\n",
    "\n",
    "_Optional:_ Die schwarzen Zahlen in den K√§stchen sind bei gewissen Farben (z.B. Dunkelblau) schlecht lesbar. √Ñndere deine Funktion so, dass die Farbe der Zahlen sind √§ndert anhand des Hintergrunds. Tipp: Mit `cm.max().max() / 2` kann mittelpunkt aller Werte in der Confusion Matrix berechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec937a45df6eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df, save_to: Path | str | None):\n",
    "    labels = ['50Sign', 'ClearSign', 'NoSign', 'StopSign'] # hardcoded\n",
    "    cm = pd.DataFrame(0, index=labels, columns=labels)  # Eine leere Tabelle f√ºr unsere Confusion Matrix\n",
    "\n",
    "    # üìù Aufgabe: Vervollst√§ndige die Funktion:\n",
    "\n",
    "\n",
    "\n",
    "example_df = pd.read_csv('../assets/example_model_output.csv')\n",
    "plot_confusion_matrix(example_df, None)\n",
    "# Die confusion Matrix sollte wie folgt aussehen:\n",
    "#       2 0 0 1\n",
    "#       0 2 0 0\n",
    "#       0 0 2 0\n",
    "#       1 0 0 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143031e8f72bc4f",
   "metadata": {},
   "source": [
    "## Modell\n",
    "\n",
    "Jetzt sind wir bereit das Schilder-Modell zu trainieren. Unser Modell gliedert sich in zwei Teile:\n",
    "1. **Feature Extractor:** Seine Aufgabe ist es aus den Pixel des Bildes Muster und Eigenschaften (sogenannte Features) zu lernen. Er besteht aus mehreren [CNN Layers](https://de.wikipedia.org/wiki/Convolutional_Neural_Network) hintereinander und ist sehr √§hnlich zu den ersten Layers des Fahrmodells.\n",
    "2. **Classifier:** Im Gegensatz zum Fahrmodell m√∂chten wir jetzt ein Modell welches Klassen (unsere Schilder) vorhersagt. Dazu verwenden wir ein kleines lineares Netz welches aus den Features die Klasse vorhersagt.\n",
    "\n",
    "### Vorgehen\n",
    "1. Modell Architektur definieren\n",
    "2. Trainingsdaten laden\n",
    "3. Trainer erstellen und Modell trainieren\n",
    "4. Auswerten und optimieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d940326214b5da",
   "metadata": {},
   "source": [
    "Bevor wir beginnen importieren wir wieder einige Packages und testen ob wir eine GPU zur verf√ºgung haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b29250134dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelSummary\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from aic.sign.model import LightningModel\n",
    "from aic.sign.dataloader import load_dataset\n",
    "from aic.logger import DictLogger\n",
    "from aic.sign.test import test_model\n",
    "from aic.runs import create_run_dir\n",
    "\n",
    "print(f'GPU (Cuda) is available: {torch.cuda.is_available()}')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac317389873c6ff",
   "metadata": {},
   "source": [
    "### Modell Architektur\n",
    "Das folgende Modell ist ein guter Startpunkt. Du darfst es gerne anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a1598d121f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineSignModel(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(BaselineSignModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                conv1 = nn.Conv2d(3, 12, 3, 2, 1),\n",
    "                batch1 = nn.BatchNorm2d(12),\n",
    "                relu1 = nn.ReLU(),\n",
    "\n",
    "                conv2 = nn.Conv2d(12, 16, 3, 2, 1),\n",
    "                batch2 = nn.BatchNorm2d(16),\n",
    "                relu2 = nn.ReLU(),\n",
    "\n",
    "                conv3 = nn.Conv2d(16, 32, 3, 1, 1),\n",
    "                batch3 = nn.BatchNorm2d(32),\n",
    "                relu3 = nn.ReLU(),\n",
    "\n",
    "                conv4 = nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                batch4 = nn.BatchNorm2d(32),\n",
    "                relu4 = nn.ReLU(),\n",
    "\n",
    "                pool = nn.MaxPool2d(2),\n",
    "\n",
    "                conv5 = nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                batch5 = nn.BatchNorm2d(32),\n",
    "                relu5 = nn.ReLU(),\n",
    "\n",
    "                average_pooling = nn.AdaptiveAvgPool2d(1),\n",
    "\n",
    "                flatten = nn.Flatten(),\n",
    "                dropout = nn.Dropout(0.2),\n",
    "                linear = nn.Linear(32, num_classes),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59301b73ac089bc0",
   "metadata": {},
   "source": [
    "Vor dem Training erstellen wir einen Ordner in den wir das Modell sowie die Auswertung speichern k√∂nnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295e0d456e52c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_directory = create_run_dir('../runs/sign/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2972dac7f782415",
   "metadata": {},
   "source": [
    "Um das Training zu vereinfachen, laden wir unser Pytorch Modell als Pytorch Lightning Modell. Zus√§tzlich definieren wir die Hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde08a9f5cb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model = BaselineSignModel(num_classes=4)\n",
    "lightning_model = LightningModel(model, epochs=EPOCHS, lr=LEARNING_RATE)\n",
    "\n",
    "logger = DictLogger()\n",
    "trainer = Trainer(max_epochs=EPOCHS, callbacks=[logger])\n",
    "\n",
    "ModelSummary().on_fit_start(Trainer(), lightning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b115fca8f1203bb",
   "metadata": {},
   "source": [
    "Jetzt sind wir bereit unser Modell zu trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218e5fa27412948",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.set_float32_matmul_precision('medium')  # Beschleunigt das Training auf Kosten leicht geringerer Pr√§zision\n",
    "\n",
    "trainer.fit(lightning_model, train_data, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ca6b022f89744",
   "metadata": {},
   "source": [
    "### Training auswerten\n",
    "\n",
    "### üìù Aufgabe 4.7\n",
    "Um das Training beurteilen zu k√∂nnen m√ºssen wir die geloggten Metriken auswerten. In `logger.metrics` sind f√ºr jede Epoche folgende Metriken gespeichert: Validation Loss, Validation Accuracy, Training Loss und Training Accuracy. Plotte die 4 Metriken mit Matplotlib und vergiss nicht den Plot in der run directory zu speichern.\n",
    "\n",
    "*Tipp: Es kann sein, dass nicht alle Metriken in allen Epochen geloggt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ade57c40abb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Aufgabe: Plotte die Metriken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2e240dd01dd7c",
   "metadata": {},
   "source": [
    "### Modell testen\n",
    "Die Trainingsmetriken zeigen uns schon auf wie gut unser Training geklappt hat. Um einen noch besseren Einblick zu bekommen wie gut das trainierte Modell tats√§chlich ist, k√∂nnen wir es auf einem Dateset testen. Dazu verwendest du am besten Daten welche das Modell noch nicht gesehen hat, also Bilder die nicht w√§hrend dem Training verwendet wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c320294e6dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_dataset('./data/test', batch_size=1, split_ratio=None, augment=False)\n",
    "print(f'testing with {len(test_data)} images...')\n",
    "\n",
    "metrics = test_model(lightning_model, test_data, device)\n",
    "\n",
    "# üìù Aufgabe: Verschiedene Tests implementieren, z.B:\n",
    "#   - Confusion Matrix\n",
    "#   - Bilder und Predictions manuell anschauen\n",
    "#   - ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda85f0b18dbdfdd",
   "metadata": {},
   "source": [
    "### Modell Export\n",
    "Um das Modell auf dem Auto laufen lassen zu k√∂nnen, m√ºssen wir es exportieren. Dazu verwenden wir wieder das ONNX-Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214fc02721a0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SignModel.onnx'\n",
    "model_export_path = run_directory / model_name\n",
    "\n",
    "batch = next(iter(train_data))\n",
    "images, _, _ = batch\n",
    "example_image = images[:1]  # onnx ben√∂tigt ein Beispielinput um das Modell exportieren zu k√∂nnen\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model.to('cpu'),\n",
    "        example_image,\n",
    "        model_export_path,\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80133db68fe31d9",
   "metadata": {},
   "source": [
    "## Optimieren\n",
    "Nachdem wir unser erstes Modell trainiert und getestet haben ist es wahrscheinlich noch nicht perfekt. Machine Learning lebt von einem iterativen Prozess der Optimierung. Jetzt geht es darum das Modell zu verbessern. Wir starten immer mit einem trainierten Modell, √§ndern etwas (Daten, Architektur, Parameter, ...), trainieren, testen und vergleichen. Dieser Prozess wird oft auch ein Experiment genannt. Zwischen zwei Experimenten sollte nicht zu viel ver√§ndert werden sonst ist es schwierig herauszufinden welche √§nderung was bewirkt, oder zwei √§nderungen heben sich gegenseitig auf.\n",
    "\n",
    "- Hyperparameter\n",
    "    - Learning Rate\n",
    "    - Epochen\n",
    "    - Batch Size\n",
    "- Modell Architektur\n",
    "    - Anzahl Convolution Layers (Feature Extractor)\n",
    "    - Anzahl Linear Layers (Classifier)\n",
    "    - Anzahl Channels in den Layers\n",
    "    - Stride Gr√∂sse in den Layers\n",
    "    - H√§ufigkeit der Pooling Operationen\n",
    "- Verschiedene Augmentationen (ohne, mit, verschiedene transforms, verschieden stark)\n",
    "- Unterschiedliche Datensets (gr√∂sser, kleiner)\n",
    "\n",
    "Wichtig: Zum Optimieren betrachten wir 2 Aspekte.\n",
    "1. Trainingsfortschritt:\n",
    "    - Lernt das Modell √ºberhaupt etwas? -> Architektur\n",
    "    - Lernt das Modell genug schnell / zu schnell? -> Learning Rate\n",
    "    - Hat das Modell schon lange fertig gelernt oder gab es am Schluss noch Verbesserungen? -> Epochen\n",
    "    - Ist der Train-loss nahe bei null, der Validation-loss aber hoch oder steigt sogar? -> Overfitting\n",
    "2. Modell Performance:\n",
    "    - Confusion Matrix\n",
    "    - Manuelle inspektion\n",
    "    - Weitere Metriken, z.B. Accuracy (% richtige predictions), [F1](https://en.wikipedia.org/wiki/F-score)\n",
    "\n",
    "\n",
    "**Tipps:**\n",
    "- Grunds√§tzlich kann ein gr√∂sseres Modell (mehr Layers) mehr lernen. Falls die Anzahl der Trainingsdaten aber klein ist bringt es oft wenig das Modell zu vergr√∂ssern (overfitting). Dazu kommt, dass das Modell auf dem Auto (Raspberry Pi 4) laufen muss. Ein sehr grosses Modell (>100'000 Parameter) l√§uft vielleicht mit wenigen Frames pro Sekunde und kann daher nicht schnell genug auf die Linie reagieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bbf850da799a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
