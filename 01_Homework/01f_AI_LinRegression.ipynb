{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Framework kennenlernen / Aufgaben\n",
    "\n",
    "In diesem Notebook werden wir unser AI Wissen anwenden um einfach Aufgaben zu lösen und dabei gerade das AI Framework **PyTorch** kennenlernen.\n",
    "\n",
    "Eine Einführung und Hilfe erhaltet ihr unter [pytorch.org/tutorials/beginner/basics/intro](https://pytorch.org/tutorials/beginner/basics/intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineare Regression mit Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten laden und betrachten\n",
    "\n",
    "Wir werden das `california housing` dataset verwenden, dass schon automatisch auf Colab zur Verfügung steht.\n",
    "\n",
    "Wir werden für das Laden dieser Daten die Library `pandas` verwenden. Es arbeitet mit sogenannten `DataFrame`, diese kann man sich wie eine Excel-Tabelle mit Zeilen und Spalten vorstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Daten als DataFrame (df) einlesen\n",
    "df = pd.read_csv('sample_data/california_housing_train.csv')\n",
    "\n",
    "# Wir geben einige Infos über das Datenset aus (Anzahl der Zeilen, Name der Spalten, Datentypen, etc.)\n",
    "df.info()\n",
    "\n",
    "# Es gibt also insgesamt 9 Spalten und Total 17000 Zeilen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein pandas DataFrame ist eine Datenstruktur in Python, die wie eine Tabelle aussieht, ähnlich wie in Excel: sie hat Zeilen und Spalten. Jede Spalte kann verschiedene Datentypen enthalten (Zahlen, Text, Datumswerte). Damit kannst du Daten einfach speichern, filtern, sortieren, berechnen und analysieren. Man erstellt ein DataFrame meistens aus Listen, Dictionaries oder Dateien (z.B. wie oben aus einem CSV).\n",
    "\n",
    "Schauen wir uns mal die ersten fünf Zeilen (rows) unserer Tabelle an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit das Training etwas schneller geht, reduzieren wir die Anzahl der Zeilen auf 30% (5100 Zeilen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 10_000:\n",
    "    df = df.sample(frac=0.3, random_state=42)\n",
    "print('Neue Anzahl Zeilen:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten visualisieren\n",
    "\n",
    "Für das Visualisieren verwenden wir die Library `matplotlib`, welche wir schon in den Python Aufgaben gesehen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Wir betrachten den Zusammenhang zwischen den Einkommen (median_income) und den Hauspreisen (median_house_value)\n",
    "plt.scatter(\n",
    "    x=df['median_income'],\n",
    "    y=df['median_house_value'],\n",
    "    s=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.xlabel('x = median_income')\n",
    "plt.ylabel('y = median_house_value')\n",
    "plt.title('Zusammenhang zwischen Einkommen und Hauspreisen');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus dem Bild können wir sehen, dass es einen Zusammenhang zwischen den beiden Variablen gibt. Wir können sagen: Leute, die mehr verdienen, wohnen in teureren Häusern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Lineare Regression\n",
    "\n",
    "Wie in der Einführung erwähnt, geht es bei der Regression darum, eine optimale Linie zu finden, welche insgesamt am nächsten an den Punkten liegt (der Fehler möglichst gering ist). Bei der linearen Regression, ist das eine simple Linie.\n",
    "\n",
    "![](../assets/01_lin_reg.png)\n",
    "\n",
    "In unserem Beispiel, wollen wir anhand des Einkommens (x) vorhersagen, wie teuer das Haus ist (y) in dem die Person wohnt. Die Linie zu finden, dürfte schon fast von Auge möglich sein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten vorbereiten\n",
    "\n",
    "Daten werden in PyTorch in sogenannten Tensor-Objekten `torch.Tensor` gespeichert. Als erstes werden wir die Daten in PyTorch Tensors umwandeln. \n",
    "\n",
    "Wir betrachten die Spalte `median_income` als X (Eingabe, Feature) und wollen den `median_house_value` als y (Zielwert) vorhersagen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Tensor-Objekte erstellen (typ = 32 bit float) sowie den Datentyp festlegen\n",
    "# Wir verwenden nun ALLE Features (alle Spalten außer median_house_value)\n",
    "feature_columns = ['housing_median_age', 'total_rooms', \n",
    "                   'total_bedrooms', 'population', 'households', 'median_income']\n",
    "X = torch.tensor(df[feature_columns].values, dtype=torch.float32)\n",
    "y = torch.tensor(df[['median_house_value']].values, dtype=torch.float32)\n",
    "\n",
    "# Betrachen wir, was hier rauskommt. Es sind zwei Tensors mit allen Features und dem Zielwert\n",
    "print('X (all features):', X)\n",
    "print('Y (median_house_value):', y)\n",
    "\n",
    "# Wir können auch schauen, welche Dimensionen (Shape) die Tensoren haben.\n",
    "# Die erste Dimension ist die Anzahl der Zeilen/Datenpunkte und die zweite Dimension ist die Anzahl der Features (8)\n",
    "print('X Shape:', X.shape)\n",
    "print('y Shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling**\n",
    "\n",
    "Für die Analyse, ist es wichtig, dass die Werte in einem genormten Bereich sind. Oft wird der Bereich -1 bis 1 oder 0 bis 1 verwendet. Wir wollen unsere Daten in den Bereich zwischen 0 und 1 bringen. Dazu können wir alle Daten durch den maximalen Wert teilen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / X.max()\n",
    "y = y / y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beispiel:**\n",
    "\n",
    "Daten: [1, 2, 5, 8, 10]\n",
    "\n",
    "max = 10\n",
    "\n",
    "map:\n",
    "- 1 / 10 = 0.1\n",
    "- 2 / 10 = 0.2\n",
    "- 5 / 10 = 0.5\n",
    "- 8 / 10 = 0.8\n",
    "- 10 / 10 = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets, Dataloader\n",
    "\n",
    "In PyTorch gibt es zwei wichtige Konzepte, die Datasets und Dataloaders.\n",
    "\n",
    "#### Datasets\n",
    "Wir werden ein Dataset erstellen, welches die Daten enthält.\n",
    "Ein Datenset speichert die Daten und gibt diese mit __getitem__ zurück. Zusätzlich kann mit __len__ die Anzahl Daten im Datenset abgefragt werden. Wenn wir unser eigenes Datenset erstellen \"erben\" wir von der torch `Dataset` Klasse und müssen dann noch diese beiden Funktionen selber programmieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class HousingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    # Die Funktion kann anschliessen mit len(dataset) aufgerufen werden\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # Die Funktion kann anschliessend mit dataset[index] aufgerufen werden und gibt X und y zurück\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt müssen wir unsere Daten (X und y) noch in unser Housing-Dataset laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HousingDataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train/Test Set**\n",
    "\n",
    "Wir werden nun dieses Datenset aufteilen, und zwar in Daten, welche wir fürs Trainieren verwenden (80%) und Daten, welche wir nur für die Validierung verwenden (20%).\n",
    "\n",
    "Das Training-Datenset verwenden wir um unser Modell zu trainieren, das Validierung-Datenset verwenden wir nach dem Training um das Modell zu testen. Um eine aussagekräftige Test-Genauigkeit zu bekommen ist es wichtig, dass Modell mit Daten zu testen welche es nicht schon während dem Training gesehen und gelernt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset=dataset, lengths=[0.8, 0.2])\n",
    "print('train:', len(train_dataset), '| val:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader\n",
    "Ein Datenloader teilt die Daten in Batches auf. Ein Batch ist eine Gruppe von Datepunkten. Es ist viel schneller und oft auch genauer ein Modell mit Batches zu trainieren statt mit einzelnen Daten. Wir verwenden das `shuffle` Argument, welches die Daten jedes mal zufällig mischt. Das macht für das Training Sinn, beim validieren (testen) wollen wir immer das genau gleiche Datenset, daher mischen wir dort nicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128  # Unser Batch Size ist 128, was bedeutet, dass wir 128 Datenpunkte gleichzeitig in das Netzwerk füttern\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vorbereiten\n",
    "\n",
    "Um das Training zu erleichtern, benützten wir die Python Bibliothek [**PyTorch Lightning**](https://lightning.ai/). Lightning stellt Funktionen zur Verfügung, die wir sonst alle selbst schreiben müssten.\n",
    "\n",
    "Wir müssen als ersten unser Neuronales Netz definieren, welches wir benützten wollen.  \n",
    "Dazu benützten wir ein Lightning-Module in welchem wir die Schichten (Layers) unseres neuronalen Netzes definieren.  \n",
    "Wir starten mit einem ganz simplen Netz mit nur einem Layer und einem Neuron.\n",
    "\n",
    "Der Code unten macht sehr viel und ist relativ komplex. Überflieg ihn, es ist oke, wenn du das meiste nicht verstehst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "class LinearRegression(LightningModule):\n",
    "    # Ein Lightning Modul definiert in seiner __init__ Funktion die Schichten des Modells\n",
    "    def __init__(self, lr = 0.001):\n",
    "        super().__init__()\n",
    "        # Unser Modell besteht nun aus mehreren Schichten:\n",
    "        # 1. Schicht: 8 Eingabe-Features -> 128 Neuronen (versteckte Schicht)\n",
    "        self.layer1 = torch.nn.Linear(in_features=6, out_features=128)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        \n",
    "        # 2. Schicht: 128 Neuronen -> 512 Neuronen (zweite versteckte Schicht)\n",
    "        self.layer2 = torch.nn.Linear(in_features=128, out_features=512)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "        # 3. Schicht: 512 Neuronen -> 128 Neuronen (dritte versteckte Schicht)\n",
    "        self.layer3 = torch.nn.Linear(in_features=512, out_features=128)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "\n",
    "        # 4. Schicht 128 Neuronen -> 16 Neuronen (4. versteckte Schicht)\n",
    "        self.layer4 = torch.nn.Linear(in_features=128, out_features=32)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        \n",
    "        # 5. Schicht: 32 Neuronen -> 1 Output (Vorhersage)\n",
    "        self.layer5 = torch.nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "        # Wir speichern die Lernrate\n",
    "        self.lr = lr\n",
    "\n",
    "        # Folgende Metriken wollen wir loggen:\n",
    "        self.logged_metrics = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"train_step\": [],\n",
    "            \"train_epoch\": [],\n",
    "            \"val_step\": [],\n",
    "            \"val_epoch\": [],\n",
    "            \"weights\": [],\n",
    "        }\n",
    "\n",
    "    # In der forward Funktion definieren wir, wie die Eingabe durch das Modell geht (x ist ein Batch von Daten)\n",
    "    def forward(self, x):\n",
    "        # Erste Schicht + Aktivierung\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        # Zweite Schicht + Aktivierung\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Zweite Schicht + Aktivierung\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        # Zweite Schicht + Aktivierung\n",
    "        x = self.layer4(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        # Ausgabe-Schicht (keine Aktivierung)\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "\n",
    "    # In der training_step Funktion definieren wir, was in einem Trainingsschritt passiert\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        # Hole die Eingabe (x) und den Zielwert (y_true) aus dem Batch\n",
    "        x, y = train_batch\n",
    "        # Mache eine Vorhersage (y_pred) mit dem Modell\n",
    "        y_pred = self(x)\n",
    "        # Berechne den Fehler (loss) zwischen Vorhersage und Zielwert\n",
    "        loss = torch.nn.functional.mse_loss(input=y_pred, target=y)\n",
    "        # Logge den Fehler, damit wir den Trainingsfortschritt beobachten können\n",
    "        self.logged_metrics[\"train_loss\"].append(loss.item())\n",
    "        self.logged_metrics[\"train_step\"].append(self.global_step)\n",
    "        self.logged_metrics[\"train_epoch\"].append(self.current_epoch)\n",
    "        return loss\n",
    "\n",
    "    # In der validation_step Funktion überprüfen wir, wie gut unsere Modell auf den Testdaten funktioniert\n",
    "    # Inhaltlich ist es fast identisch mit dem training_step. Nur die Daten sind anders\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_pred = self(x)\n",
    "        loss = torch.nn.functional.mse_loss(input=y_pred, target=y)\n",
    "        self.logged_metrics[\"val_loss\"].append(loss.item())\n",
    "        self.logged_metrics[\"val_step\"].append(self.global_step)\n",
    "        self.logged_metrics[\"val_epoch\"].append(self.current_epoch)\n",
    "        # Für komplexere Modelle speichern wir nur den ersten Layer's Gewichte\n",
    "        self.logged_metrics[\"weights\"].append((self.layer1.weight[0, 0].item(), self.layer1.bias[0].item()))\n",
    "        return loss\n",
    "\n",
    "    # Der Optimizer optimiert unser Modell. Hier wird auch die Lernrate (lr) definiert\n",
    "    # Die Lernrate gibt an, wie gross die Schritte sind, die der Optimizer macht. Dazu mehr später\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Das Ziel beim Training ist es, dass das Modell lernt anhand vom Input `X`, den Output `y` vorherzusagen. Dazu zeigen wir dem Modell unsere X Werte, es macht eine Vorhersage, wir vergleichen die Vorhersage (`y_pred`) mit dem tatsächlichen Wert `y` und das Modell passt sich leicht an, damit beim nächsten mal die Vorhersage etwas näher bei `y` liegt. Die Differenz zwischen der vorhersage `y_pred` und `y` nennen wir den \"loss\" (Deutsch: Fehler).\n",
    "\n",
    "Nun zum eigentlichen Training: Die `Trainer` Klasse übernimmt für uns das Training. Mit `max_epochs` geben wir an, wie viel mal wir durch das gesamte Trainings-Datenset iterieren wollen.\n",
    "\n",
    "Ist dieser Wert zu klein, dann kommt es zu **Unterfitting** (das Modell lernt zu wenig), ist dieser zu hoch, zu **Overfitting** (das Modell lernt die Daten auswendig). In der Praxis müssen wir also den optimalen Wert für die Anzahl Epochen finden.\n",
    "\n",
    "Die Lernrate definiert wie stark sich das Modell bei einem Trainingsschritt anpasst. Bei einer hohen Lernrate lernt dass Modell schneller aber unruhiger, bei einer kleinen dauert das Training länger ist aber stabiler. Auch hier ist es eine Kunst einen passenden Wert zu finden.\n",
    "\n",
    "Die **Lernrate** und die **Anzahl Epochen** sind die wichtigsten Parameter um ein schnelles Training mit guten Ergebnissen zu erhalten. Solche Parameter werden **Hypterparameter** genannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erstellen einen Trainer und eine Instanz unseres Modells von oben. Da es sich bei unserem Modell um ein sehr einfaches handelt, trainieren wir auf der CPU (`accelerator`). Grössere Modelle, wie wir sie später bauen, sollten auf der GPU trainiert werden, sonst dauert das Training sehr lange..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(max_epochs=EPOCHS, accelerator='gpu')\n",
    "model = LinearRegression(lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir unser Modell trainieren. Die `fit` Funktion ruft wiederholt die `training_step` Funktion unseres Modells auf und übergibt ihr die X und y Daten aus unserem Datenset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nun können wir uns die Trainings- und Validierungsfehler anschauen\n",
    "train_loss = model.logged_metrics['train_loss']\n",
    "val_loss = model.logged_metrics['val_loss']\n",
    "\n",
    "plt.figure(figsize=[10, 7])\n",
    "plt.plot(train_loss, label='train_loss')\n",
    "plt.plot(\n",
    "    [i * (len(train_loss) / len(val_loss)) for i in range(len(val_loss))],\n",
    "    val_loss,\n",
    "    label='val_loss',\n",
    ")\n",
    "# Die X-Achse mit Epochs beschriften.\n",
    "plt.xticks(\n",
    "    ticks=[i * (len(train_loss) // 10) for i in range(11)],\n",
    "    labels=[f'{i*(EPOCHS//10)}' for i in range(11)],\n",
    ")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Fehler)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, dass der Fehler zuerst schnell abnimmt, sich jedoch im Verlauf immer langsamer verringert. Sobald der Loss gegen Ende nicht weiter sinkt, deutet das darauf hin, dass das Modell nichts Neues mehr lernen kann.\n",
    "\n",
    "Da unser Modell nun 8 Eingabe-Features verwendet, können wir nicht mehr einfach eine Linie plotten. Stattdessen visualisieren wir die Vorhersagen vs. die echten Werte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir machen Vorhersagen auf dem Validierungsset\n",
    "model.eval()  # Modell in Evaluierungsmodus setzen\n",
    "with torch.no_grad():\n",
    "    X_val = val_dataset.dataset.X[val_dataset.indices]\n",
    "    y_val = val_dataset.dataset.y[val_dataset.indices]\n",
    "    y_pred = model(X_val)\n",
    "\n",
    "# Plotten: Echte Werte vs. Vorhersagen\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val.numpy(), y_pred.numpy(), alpha=0.5, s=10)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfekte Vorhersage')\n",
    "plt.xlabel('Echte Werte (y)')\n",
    "plt.ylabel('Vorhersagen (y_pred)')\n",
    "plt.title('Vorhersagen vs. Echte Werte')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Je näher die Punkte an der roten Linie sind, desto besser die Vorhersage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was hat das Model gelernt?\n",
    "\n",
    "Da unser Modell nun mehrere Schichten und viele Gewichte hat (8x64 + 64x32 + 32x1 = 2592 Parameter!), können wir nicht mehr einfach das Gewicht und Bias anzeigen wie bei einer einfachen Geraden.\n",
    "\n",
    "Stattdessen können wir uns die **Feature Importance** anschauen - welche Eingabe-Features haben den größten Einfluss auf die Vorhersage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy falls noch nicht importiert\n",
    "import numpy as np\n",
    "\n",
    "# Feature Importance: Durchschnittliche absolute Gewichte der ersten Schicht\n",
    "feature_names = ['housing_median_age', 'total_rooms', \n",
    "                 'total_bedrooms', 'population', 'households', 'median_income']\n",
    "\n",
    "# Berechne die durchschnittliche absolute Gewichte für jedes Feature\n",
    "weights = model.layer1.weight.detach().cpu().numpy()\n",
    "feature_importance = np.abs(weights).mean(axis=0)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, feature_importance)\n",
    "plt.xlabel('Durchschnittliche Absolute Gewichtung')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance - Welche Features sind am wichtigsten?')\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Je größer der Balken, desto wichtiger ist das Feature für die Vorhersage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besonders spannend ist es zu sehen, wie sich das Modell während dem Training verändert hat. \n",
    "\n",
    "Da wir nun ein komplexes Modell mit vielen Parametern haben, visualisieren wir stattdessen:\n",
    "1. Wie sich die **Gewichte eines einzelnen Neurons** über das Training ändern\n",
    "2. Wie sich die **Vorhersagequalität** verbessert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualisierung 1: Entwicklung der Gewichte während des Trainings\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Gewicht und Bias eines einzelnen Neurons über Zeit\n",
    "weights_history = model.logged_metrics['weights']\n",
    "weights = [w[0] for w in weights_history]\n",
    "biases = [w[1] for w in weights_history]\n",
    "\n",
    "ax1.plot(weights, label='Gewicht (erstes Neuron)', alpha=0.7)\n",
    "ax1.plot(biases, label='Bias (erstes Neuron)', alpha=0.7)\n",
    "ax1.set_xlabel('Validation Steps')\n",
    "ax1.set_ylabel('Wert')\n",
    "ax1.set_title('Entwicklung von Gewicht und Bias während Training')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Vergleich Train vs Validation Loss\n",
    "ax2.plot(model.logged_metrics['train_loss'], label='Train Loss', alpha=0.7)\n",
    "# Resample val_loss to match train_loss length for visualization\n",
    "val_indices = np.linspace(0, len(model.logged_metrics['train_loss'])-1, len(model.logged_metrics['val_loss']))\n",
    "ax2.plot(val_indices, model.logged_metrics['val_loss'], label='Validation Loss', alpha=0.7, linewidth=2)\n",
    "ax2.set_xlabel('Training Steps')\n",
    "ax2.set_ylabel('Loss (MSE)')\n",
    "ax2.set_title('Training- vs. Validation-Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie weiter?\n",
    "\n",
    "Versucht nun folgendes, um das Model zu verbessern und eure Wissen über neuronale Netzwerke zu vertiefen\n",
    "\n",
    "### Basics:\n",
    "\n",
    "- Welcher Einfluss haben folgende Hyperparameter auf das Training und den Trainings-Erfolg? Wie verändert sich die Lernkurve?\n",
    "  - Anzahl Epochen `EPOCHS`\n",
    "  Antwort:\n",
    "    - Bei wenigen Epochen hat das Model zu wenig zeit um zu Lernen.\n",
    "    - Bei vielen Epochen lernt das Model die Daten auswendig\n",
    "    - Bei der Optimalen Anzahl hat das Modell genug Zeit zum Lernen hat aber zu wenig das es die Daten auswendig lernt.\n",
    "\n",
    "\n",
    "  - Learnrate `lr` in der Funktion `configure_optimizers()`\n",
    "  Antwort:\n",
    "    - Wenn die Lernrate klein eingestellt ist lernt das Modell sehr langsam.\n",
    "    - Wenn die Lernrate hoch eingestellt ist lernt das Modell instabil, was zu einem höheren Verlust wird.\n",
    "    - Wenn die Lernrate korrekt eingestellt ist lernt es gleichmässig und verliert nicht übermässig Daten.\n",
    "\n",
    "  - Batch Grösse `BATCH_SIZE`\n",
    "  Antwort:\n",
    "    - Kleine Batches bringen mehr Updates pro Epoche, was aber zu einer Unruhigen Lernkurve führt.\n",
    "    - Grosse Batches bringen weniger Updates pro Epoche was zu Langsamen lernen führt.\n",
    "    - Mit der optimalen Anzahl ist es stabil und schnell.\n",
    "\n",
    "### Fortgeschritten:\n",
    "\n",
    "- Füge mehr Inputs hinzu. Gehe davon wie folgt vor\n",
    "  - Plotte den Zusammenhang zwischen den anderen Inputs (z.b. `total_rooms`) und unserem Ziel (`median_income`). Siehst du einen Zusammenhang?\n",
    "  - Wo im Notebook musst du Änderungen machen, damit das Model mehr Inputs verwendet? Tipp: Es sind zwei Stellen\n",
    "\n",
    "### Sehr Fortgeschritten:\n",
    "- Erweitere das neuronale Netzwerk (mit einem Input), in dem du mehrere Layer (Schichten), gefolgt von einer Aktivierungsfunktion (ReLU, Details folgen in der kommenden Woche) hinzufügt. Kann das Netzwerk nun auch nicht-lineare Zusammenhänge lernen (nicht nur eine Gerade, sondern Linie mit Kurven)?\n",
    "  - Tipp: Hier ein Beispiel eines Netzwerk mit 2 Layers (Schichten):\n",
    "    ```python\n",
    "    # in __init__\n",
    "    self.layer = torch.nn.Linear(in_features=1, out_features=1)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.layer2 = torch.nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    # in forward:\n",
    "    x = self.layer(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.layer2(x)\n",
    "    return x\n",
    "    ```\n",
    "  - Achtung: Die Visualisierung der Gewichte und Bias macht keinen Sinn mehr wenn du weitere Layers hinzufügst."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
